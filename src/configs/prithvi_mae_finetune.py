from __future__ import annotations

import dataclasses
from dataclasses import dataclass

from data.mae_datamodule import MAEDatamoduleConfig


@dataclass
class Config:
    model: ModelConfig
    datamodule: MAEDatamoduleConfig
    train: TrainConfig


@dataclass
class ModelConfig:
    num_frames: int  # input frames per prediction
    output_embed_dim: int
    patch_height: int
    patch_width: int
    fcn_out_channels: int
    fcn_num_convs: int
    fcn_dropout: float

    num_classes: int | None = None  # set dynamically from dataset tag mapping
    embed_dim: int = 768  # fixed prithvi output embedding dim


@dataclass
class TrainConfig:
    # optimizer
    lr: float
    weight_decay: float
    betas: tuple[float, float]

    # lr scheduler
    use_lr_scheduler: bool
    lr_scheduler_type: str
    lr_step_size: int
    lr_gamma: float
    weight_decay: float

    float32_matmul_precision: str

    # compile
    compile_mode: str
    compile_fullgraph: bool
    compile_disable: bool

    # trainer
    devices: int
    precision: str
    overfit_batches: float

    # logger
    use_wandb_logger: bool
    log_img_in_train: bool
    project_name: str
    wandb_entity: str | None = None
    run_name: str | None = None  # set from script / autogenerated
    tags: list[str] = dataclasses.field(default_factory=list)

    seed: int = 42


# CONFIG = Config(
#     model=ModelConfig(
#     ),
#     datamodule=MAEDatamoduleConfig(
#         dataset_cfg=MAEDatasetConfig(aoi="at"),
#         batch_size=32,
#         num_workers=1,
#         pin_memory=True,
#         augment=True,
#         data_split=(0.8, 0.2, 0.0),
#         val_batch_size_multiplier=2,
#         # transforms
#         random_crop_size=224,
#     ),
#     train=TrainConfig(
#         project_name="prithvi-mae-finetune",
#         lr=1.5e-05,
#         weight_decay=0.05,
#         betas=(0.9, 0.999),
#         float32_matmul_precision="medium",
#         compile_mode="max-autotune",
#         compile_fullgraph=True,
#         compile_disable=False,
#         devices=1,
#         precision="bf16-mixed",
#         overfit_batches=0.0,
#         use_wandb_logger=True,
#         tags=[],
#         log_img_in_train=False,
#         use_lr_scheduler=False,
#         lr_scheduler_type="StepLR",
#         lr_step_size=10,
#         lr_gamma=0.1,
#     ),
# )


def debug(config: Config) -> Config:
    config.train.devices = 1
    config.datamodule.batch_size = 1
    config.train.compile_disable = True
    config.train.log_img_in_train = True
    config.train.tags.append("debug")
    return config


def overfit(config: Config) -> Config:
    config.train.overfit_batches = 1
    config.datamodule.augment = False
    config.train.log_img_in_train = True
    config.train.tags.append("overfit")
    return config
